{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fef6a0f-d919-4c87-b582-eacd2b852cee",
   "metadata": {},
   "source": [
    "# 302 Spark optimizations\n",
    "\n",
    "The goal of this lab is to understand some of the optimization mechanisms of Spark.\n",
    "\n",
    "- [Spark programming guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n",
    "- [RDD APIs](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/RDD.html)\n",
    "- [PairRDD APIs](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/PairRDDFunctions.html)\n",
    "\n",
    "Let's start by setting the proper cluster configuration. In case of default setup (i.e., 2 [m5.xlarge](https://aws.amazon.com/it/ec2/instance-types/m5/) machines with 4 cores and 16 GB of RAM each):\n",
    "\n",
    "- 2 executors with 3 cores each (leave 1 for daemons; and there's also the AMP)\n",
    "- 8G of memory per executor (slide calculations would recommend 11G, but it exceeds YARN's default maximum allowed in this EMR cluster)\n",
    "\n",
    "Syntax details [here](https://aws.amazon.com/it/premiumsupport/knowledge-center/modify-spark-configuration-emr-notebook/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ff8ad7-b73c-4a46-9e32-92b71d6a3479",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\"executorMemory\":\"8G\", \"numExecutors\":2, \"executorCores\":3, \"conf\": {\"spark.dynamicAllocation.enabled\": \"false\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70c02bd-4c8f-4cc2-9a13-544da7c6544d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "//val bucketname = \"unibo-bd2223-egallinucci\"\n",
    "val bucketname = \"eg-myfirstbucket\"\n",
    "\n",
    "val path_weather = \"s3a://\"+bucketname+\"/datasets/weather-sample1.txt\"\n",
    "val path_weather_full = \"s3a://\"+bucketname+\"/datasets/weather-sample10.txt\"\n",
    "val path_stations = \"s3a://\"+bucketname+\"/datasets/stations.csv\"\n",
    "\n",
    "\"SPARK UI: Enable forwarding of port 20888 and connect to http://localhost:20888/proxy/\" + sc.applicationId + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7648dedd-4462-44e4-bcf7-5dc3af6f08a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "//Initialize the following objects to give some structure to the weather RDDs.\n",
    "\n",
    "case class WeatherData(\n",
    "  usaf:String,\n",
    "  wban:String,\n",
    "  year:String,\n",
    "  month:String,\n",
    "  day:String,\n",
    "  temperature:Double,\n",
    "  validTemperature:Boolean\n",
    ")\n",
    "\n",
    "object WeatherData {\n",
    "    def extract(row:String) = {\n",
    "        val usaf = row.substring(4,10)\n",
    "        val wban = row.substring(10,15)\n",
    "        val year = row.substring(15,19)\n",
    "        val month = row.substring(19,21)\n",
    "        val day = row.substring(21,23)\n",
    "        val airTemperature = row.substring(87,92)\n",
    "        val airTemperatureQuality = row.charAt(92)\n",
    "\n",
    "        new WeatherData(usaf,wban,year,month,day,airTemperature.toInt/10,airTemperatureQuality == '1')\n",
    "    }\n",
    "}\n",
    "\n",
    "case class StationData(\n",
    "  usaf:String,\n",
    "  wban:String,\n",
    "  name:String,\n",
    "  country:String,\n",
    "  state:String,\n",
    "  call:String,\n",
    "  latitude:Double,\n",
    "  longitude:Double,\n",
    "  elevation:Double,\n",
    "  date_begin:String,\n",
    "  date_end:String\n",
    ")\n",
    "\n",
    "object StationData {\n",
    "  def extract(row:String) = {\n",
    "    def getDouble(str:String) : Double = {\n",
    "      if (str.isEmpty)\n",
    "        return 0\n",
    "      else\n",
    "        return str.toDouble\n",
    "    }\n",
    "    val columns = row.split(\",\").map(_.replaceAll(\"\\\"\",\"\"))\n",
    "    val latitude = getDouble(columns(6))\n",
    "    val longitude = getDouble(columns(7))\n",
    "    val elevation = getDouble(columns(8))\n",
    "    new StationData(columns(0),columns(1),columns(2),columns(3),columns(4),columns(5),latitude,longitude,elevation,columns(9),columns(10))\n",
    "  }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4b49ee-6852-4025-9e55-3950ff937680",
   "metadata": {},
   "source": [
    "## 302-1 Simple job optimization\n",
    "\n",
    "Optimize the two jobs (avg temperature and max temperature) by avoiding the repetition of the same computations and by defining a good number of partitions.\n",
    "- See what happens when defining more/less partitions.\n",
    "- See the effects of the different repartitioning methods.\n",
    "\n",
    "Hints:\n",
    "- To change the partitioning of and RDD, you can use:\n",
    "  - ```repartition(numPartitions)```: shuffles all the data\n",
    "  - ```coalesce(numPartitions)```: minimizes data shuffling by exploiting the existing partitioning\n",
    "  - ```partitionBy(partitioner)```: shuffles all the data according to a given criteria (see 302-2 to understand how to use it)\n",
    "- Verify your persisted data in the web UI\n",
    "- Verify the execution plan of your RDDs with ```rdd.toDebugString``` (shell only) or on the web UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae20e128-aebc-4340-be2f-9da672fa81f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val rddWeather = sc.textFile(path_weather_full).map(WeatherData.extract)\n",
    "\n",
    "// Average temperature for every month\n",
    "rddWeather.\n",
    "    filter(_.temperature<999).\n",
    "    map(x => (x.month, x.temperature)).\n",
    "    aggregateByKey((0.0,0.0))((a,v)=>(a._1+v,a._2+1),(a1,a2)=>(a1._1+a2._1,a1._2+a2._2)).\n",
    "    map({case(k,v)=>(k,v._1/v._2)}).\n",
    "    collect()\n",
    "\n",
    "// Maximum temperature for every month\n",
    "rddWeather.\n",
    "    filter(_.temperature<999).\n",
    "    map(x => (x.month, x.temperature)).\n",
    "    reduceByKey((x,y)=>{if(x<y) y else x}).\n",
    "    collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377fbf30-f568-413c-9238-de139db23135",
   "metadata": {},
   "source": [
    "## 302-2 RDD preparation\n",
    "\n",
    "Check the five possibilities to prepare the Station RDD for subsequent (multiple) processing and identify the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16b6b4e-b4b6-4ca3-94bb-11b6c65c03d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.HashPartitioner\n",
    "val p = new HashPartitioner(8)\n",
    "\n",
    "val rddStation = sc.textFile(path_stations).map(StationData.extract)\n",
    "\n",
    "val rddS1 = rddStation.\n",
    "  keyBy(x => x.usaf + x.wban).\n",
    "  partitionBy(p).\n",
    "  cache().\n",
    "  map({case (k,v) => (k,(v.country,v.elevation))})\n",
    "val rddS2 = rddStation.\n",
    "  keyBy(x => x.usaf + x.wban).\n",
    "  map({case (k,v) => (k,(v.country,v.elevation))}).\n",
    "  cache().\n",
    "  partitionBy(p)\n",
    "val rddS3 = rddStation.\n",
    "  keyBy(x => x.usaf + x.wban).\n",
    "  partitionBy(p).\n",
    "  map({case (k,v) => (k,(v.country,v.elevation))}).\n",
    "  cache()\n",
    "val rddS4 = rddStation.\n",
    "  keyBy(x => x.usaf + x.wban).\n",
    "  map({case (k,v) => (k,(v.country,v.elevation))}).\n",
    "  partitionBy(p).\n",
    "  cache()\n",
    "val rddS5 = rddStation.\n",
    "  map(x => (x.usaf + x.wban, (x.country,x.elevation))).\n",
    "  partitionBy(p).\n",
    "  cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c3071b-c9ee-4c02-a85f-2800b9c4d8ed",
   "metadata": {},
   "source": [
    "## 302-3 Joining RDDs\n",
    "\n",
    "Define the join between rddWeather and rddStation and compute:\n",
    "- The maximum temperature for every city\n",
    "- The maximum temperature for every city in the UK: \n",
    "  - ```StationData.country == \"UK\"```\n",
    "- Sort the results by descending temperature\n",
    "  - ```map({case(k,v)=>(v,k)})``` to invert key with value and vice versa\n",
    "\n",
    "Hints & considerations:\n",
    "- Keep only temperature values <999\n",
    "- Join syntax: ```rdd1.join(rdd2)```\n",
    "  - Both RDDs should be structured as key-value RDDs with the same key: usaf + wban\n",
    "- Consider partitioning and caching to optimize the join\n",
    "  - Careful: it is not enough for the two RDDs to have the same number of partitions; they must have the same partitioner!\n",
    "- Verify the execution plan of the join in the web UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32bce1c-e062-478c-9c08-d9017c26a6ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val rddWeather = sc.textFile(path_weather).map(WeatherData.extract)\n",
    "val rddStation = sc.textFile(path_stations).map(StationData.extract)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c47156d-62bd-42cf-bb15-5d2496f8b882",
   "metadata": {},
   "source": [
    "## 302-4 Memory occupation\n",
    "\n",
    "Use Spark's web UI to verify the space occupied by the provided RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3068b3-f2aa-4d13-812b-7d0461a35390",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.storage.StorageLevel._\n",
    "val rddWeather = sc.textFile(path_weather_full).map(WeatherData.extract)\n",
    "\n",
    "sc.getPersistentRDDs.foreach(_._2.unpersist())\n",
    "\n",
    "val memRdd = rddWeather.repartition(14).cache()\n",
    "val memSerRdd = memRdd.map(x=>x).persist(MEMORY_ONLY_SER)\n",
    "val diskRdd = memRdd.map(x=>x).persist(DISK_ONLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c7bc50-bb59-4e70-8955-8a44d7de774d",
   "metadata": {},
   "source": [
    "## 302-5 Evaluating different join methods\n",
    "\n",
    "Consider the following scenario:\n",
    "- We have a disposable RDD of Weather data (i.e., it is used only once): ```rddW```\n",
    "- And we have an RDD of Station data that is used many times: ```rddS```\n",
    "- Both RDDs are cached (```collect()```is called to enforce caching)\n",
    "\n",
    "We want to join the two RDDS. Which option is best?\n",
    "- Simply join the two RDDs\n",
    "- Enforce on ```rddW1``` the same partitioner of ```rddS``` (and then join)\n",
    "- Exploit broadcast variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d77122-8bdd-4784-a86e-f42f2da06759",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.HashPartitioner\n",
    "\n",
    "val rddWeather = sc.textFile(path_weather_full).map(WeatherData.extract)\n",
    "val rddStation = sc.textFile(path_stations).map(StationData.extract)\n",
    "\n",
    "val p = new HashPartitioner(14)\n",
    "\n",
    "val rddW = rddWeather.\n",
    "  filter(_.temperature<999).\n",
    "  keyBy(x => x.usaf + x.wban).\n",
    "  cache()\n",
    "val rddS = rddStation.\n",
    "  keyBy(x => x.usaf + x.wban).\n",
    "  partitionBy(p).\n",
    "  cache()\n",
    "\n",
    "// Collect to enforce caching\n",
    "rddW.saveAsTextFile(\"s3a://\"+bucketname+\"/tmp/rddW\")\n",
    "rddS.saveAsTextFile(\"s3a://\"+bucketname+\"/tmp/rddS\")\n",
    "\n",
    "// Is it better to simply join the two RDDs..\n",
    "rddW.\n",
    "  join(rddS).\n",
    "  map({case(k,v)=>(v._2.name,v._1.temperature)}).\n",
    "  reduceByKey((x,y)=>{if(x<y) y else x}).\n",
    "  collect\n",
    "\n",
    "// ..to enforce on rddW1 the same partitioner of rddS..\n",
    "rddW.\n",
    "  partitionBy(p).\n",
    "  join(rddS).\n",
    "  map({case(k,v)=>(v._2.name,v._1.temperature)}).\n",
    "  reduceByKey((x,y)=>{if(x<y) y else x}).\n",
    "  collect()\n",
    "\n",
    "// // ..or to exploit broadcast variables?\n",
    "val bRddS = sc.broadcast(rddS.map(x => (x._1, x._2.name)).collectAsMap())\n",
    "val rddJ = rddW.\n",
    "  map({case (k,v) => (bRddS.value.get(k),v.temperature)}).\n",
    "  filter(_._1!=None)\n",
    "rddJ.\n",
    "  reduceByKey((x,y)=>{if(x<y) y else x}).\n",
    "  collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cc81c0-1425-4ef9-8a19-a7edca031c33",
   "metadata": {},
   "source": [
    "## 302-6 Optimizing Exercise 3\n",
    "\n",
    "Start from the result of Exercise 3; is there a more efficient way to compute the same result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47748353-fb4b-432f-af79-d1136453b956",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val rddWeather = sc.textFile(path_weather).map(WeatherData.extract)\n",
    "val rddStation = sc.textFile(path_stations).map(StationData.extract)\n",
    "\n",
    "val rddW = rddWeather.filter(_.temperature<999).keyBy(x => x.usaf + x.wban).cache()\n",
    "val rddS = rddStation.keyBy(x => x.usaf + x.wban).cache()\n",
    "\n",
    "val rdd6a = rddW.\n",
    "  join(rddS).\n",
    "  filter(_._2._2.country==\"UK\").\n",
    "  map({case(k,v)=>(v._2.name,v._1.temperature)}).\n",
    "  reduceByKey((x,y)=>{if(x<y) y else x}).\n",
    "  map({case(k,v)=>(v,k)}).\n",
    "  sortByKey(false).\n",
    "  collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}